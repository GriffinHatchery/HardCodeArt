{"name":"Hard Code Art","tagline":"This is a project that is aimed at using Python and Neural Networks to understand art.  ","body":"#_HardCodeArt_\r\n\r\n![Graph with results](https://github.com/suin3g/HardCodeArt/blob/master/examples/Disagreement.png?raw=true)\r\n\r\n![Image of program](https://github.com/suin3g/HardCodeArt/blob/master/examples/HardCodeArt.jpg?raw=true)\r\n\r\n### Background and Theory:\r\nSound is able to convey emotions that are more then just cultural. For music it seems most reasonable that this has everything to \u000Bdo with the frequencies that make it up. Using the Fourier Transform it is possible to map any function, including images,\u000B into the frequencies domain. My theory that the frequencies that make up images are responsible for the emotions that are conveyed by the image.\r\n\r\n![FFT of image](https://github.com/suin3g/HardCodeArt/blob/master/examples/FFT.jpg?raw=true)\r\n\r\n_FFT of image_\r\n\r\n### Model:\r\nTo test my theory I created a neural network using PyBrain's implementation, which I will train to recognize the adjectives people associate with art. The images will be transformed to the frequencies domain using Numpy's Fast Fourier Transform. This was used as the inputs to the Neural Network. The outputs are a set of adjectives. I narrowed down the adjectives to Sad, Happy, Serious, Whimsical, Peaceful, Intense, Angry, and Simple. Only images without recognizable objects in it will be used to insure that the emotions are caused by the colors and patterns not the objects.  \r\n\r\n![Second FFT of image](https://github.com/suin3g/HardCodeArt/blob/master/examples/FFT2.jpg?raw=true)\r\n\r\n_FFT of a seccond image_\r\n\r\n### Why Should You Care:\r\nIf the theory is right then it could be used to better understand how we interact with art. This could give insight into how we understand the world.\r\n\r\n###Data Collection:\r\n\r\nI had 15 volunteers examine 94 images. for each image I found the average and standard deviations of the number of disagreements between every pair of volunteers. This is used to compare how much each person agreed with each other. In the graph each point represents the average number of disagreements for one image and the error bars represent the standard deviation.\r\n\r\n![Disagreement Between Humans](https://github.com/suin3g/HardCodeArt/blob/master/examples/Human.png?raw=true)\r\n\r\n_Disagreement Between Humans_\r\n\r\nIt is important to notice the large standard deviation this is because humans are very subjective and often do not agree with art. The most we can hope to achieve is an AI that is able to consistently chose emotions that when compared with the humans results in points within the error bars.      \r\n\r\n###Data Compression:\r\nI am using a moving window approach to data collection. For each image I randomly choose part of the image. Either the whole image(blue), 1/4th of the image(red), or 1/16th of the image(red) is used. The part of the image used is randomized for each training epoch. \r\n\r\n![Image of program with windows](https://github.com/suin3g/HardCodeArt/blob/master/examples/HardCodeArtWindow.jpg?raw=true)\r\n\r\n_Data Collection Windows_\r\n\r\nOne of the major issues is that the amount of data in an image is huge. The Fourier Transform does not change the amount of data. Though if one takes the absolute value of the Fourier Transform of an image it exhibits rotational symmetry.\r\n\r\n![FFT of image with rotational symmetry](https://github.com/suin3g/HardCodeArt/blob/master/examples/FFT3.png?raw=true)\r\n\r\n_FFT of image with rotational symmetry_\r\n\r\nI used this fact and only considered half the FFT. I then considered only eight windows and took the advantage value of the FFT within these windows. Each Color is considered by itself. I also considered the standard deviations within each of these windows. In all this leads to 48 inputs.\r\n\r\n![FFT of image with eight key windows](https://github.com/suin3g/HardCodeArt/blob/master/examples/FFT3edit.png?raw=true)  \r\n\r\n_FFT of image with eight key windows_\r\n\r\nThere is a inner layer with 16 levels and the output layer has the 8 emotions. \r\n\r\n###Results:\r\nTo see if the training worked we must first look at an untrained network. This graph is like the one shown above with the results from an untrained network added in red. Each red point represents the average disagreement between the emotions the AI chose and each human. Because the AI is untrained it is basically making random guesses. \r\n\r\n![Disagreement Between Humans and Random](https://github.com/suin3g/HardCodeArt/blob/master/examples/Random.png?raw=true)\r\n\r\n_Random_\r\n\r\nTwo important numbers to notice are the number of points outside the standard deviation and the number of points above the human mean. 88% of the points where above above the human mean, and 52% of the points were outside the standard deviation for this untrained AI.\r\n\r\nThe Next Graph shows the results after the network was trained.\r\n\r\n![Disagreement Between Humans and Trained AI](https://github.com/suin3g/HardCodeArt/blob/master/examples/AI.png?raw=true)\r\n\r\n_Trained AI_\r\n\r\nThis is much better then random. Only 1% of the points are outside the standard deviation and 39% are above the human mean. This means that the computer is making choice similar to those that a human would. From this data set one would not be able to tell if this were done by a human or a computer. That is as good as is possible and I consider this a huge success.  \r\n\r\n### Using software:\r\nAfter downloading code you must add the images you would like to use to a file named \"images\" in the project folder. I did not include the images I used because I do not own rights to them.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}